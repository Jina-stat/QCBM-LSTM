import typing as t            # typing ëª¨ë“ˆ : íŒŒì´ì¬ì˜ íƒ€ì… íŒíŒ…(type hinting) ê¸°ëŠ¥ì„ ì œê³µ
from collections import deque # ë°í¬(deque) : ì–‘ë°©í–¥ í(queue) ìë£Œêµ¬ì¡°ë¥¼ êµ¬í˜„í•˜ëŠ” í´ë˜ìŠ¤

import numpy as np            # ìˆ˜ì¹˜ ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd           # ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ â†’ ë‚´ë¶€ì ìœ¼ë¡œëŠ” Pillow ê°€ ë¡œë“œ, Pillow ëŠ” ì‚¬ì‹¤ â€œPILâ€ API ë¥¼ ê·¸ëŒ€ë¡œ ê³„ìŠ¹í•œ í›„ì† íŒ¨í‚¤ì§€
from PIL import Image 

import selfies as sf # SELFIES ì¸ì½”ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬
from rdkit.Chem import AllChem as Chem # í™”í•™ êµ¬ì¡° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from rdkit.Chem import Draw            # í™”í•™ êµ¬ì¡° ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

from .mol_methods import * # (ì‚¬ìš©ì ì •ì˜) í™”í•™ êµ¬ì¡° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì„œë“œë“¤ 

import torch
# from orquestra.qml.api import Tensor, convert_to_numpy # ì–‘ì ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬




# SelfiesEncoding í´ë˜ìŠ¤ ì •ì˜ 
class SelfiesEncoding:
    """
        SELFIES ì¸ì½”ë”© í´ë˜ìŠ¤ ì´ˆê¸°í™”.SMILESë¥¼ SELFIESë¡œ ë³€í™˜í•˜ê³  ìˆ«ì ì¸ì½”ë”©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ì¤€ë¹„

        Args:
            filepath (str): SMILESë¥¼ í¬í•¨í•œ CSV íŒŒì¼ ê²½ë¡œ
            dataset_identifier (str): ë°ì´í„°ì…‹ ì´ë¦„ ë˜ëŠ” ì‹ë³„ì
            start_char (str): SELFIES ì‹œí€€ìŠ¤ ì‹œì‘ ë¬¸ì
            pad_char (str): SELFIES íŒ¨ë”© ë¬¸ì
            max_length (t.Optional[int], optional): ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ê°’: ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ * 1.5)
                : an optional argument to specify the maximum length of sequences.
                  If not specified, the length of the 1.5 times the longest sequence in the provided file will be used.
    """
    def __init__(
        self,
        filepath: str,
        dataset_identifier: str,
        start_char: str = "[^]",
        pad_char: str = "[nop]",
        max_length: t.Optional[int] = None,
    ):
        self.dataset_identifier = dataset_identifier
        self._filepath = filepath
        self.df = pd.read_csv(self._filepath)

        # ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.train_samples = []    # ìœ íš¨í•œ SELFIES ì‹œí€€ìŠ¤ ì €ì¥
        self.invalid_smiles_rdkit = []   # ìœ íš¨í•˜ì§€ ì•Šì€ SMILES ì €ì¥ by rdkit ìœ íš¨ì„±ê²€ì‚¬
        self.invalid_smiles_encoding = [] # ìœ íš¨í•˜ì§€ ì•Šì€ SMILES ì €ì¥ by selfies encoding process
        self.valid_smiles = []
        """
        Error 1. SELFIESì˜ ê¸°ë³¸ ì œì•½ì¡°ê±´ì¸ ìµœëŒ€ 4ê°œì˜ ê²°í•©ì„ ì´ˆê³¼í•˜ëŠ” ë¬¸ì œ ë°œìƒ
            ì˜ˆì‹œ. CN1CCC(c2nc(OCC3CCCN3C)nc3c2C#[PH](=O)N(Cc2ccccc2)C3)C1

            ìˆ˜ì • ë°©ë²• : ì œì•½ ì¡°ê±´ì˜ í•´ì œ ë˜ëŠ” ë³€ê²½
            a. ì œì•½ ì™„ì „ í•´ì œ : sf.set_semantic_constraints({})
            b. íŠ¹ì • ì›ì†Œë§Œ ì œì•½ ìˆ˜ì •
                # [PH1]ì˜ ìµœëŒ€ ê²°í•© ìˆ˜ë¥¼ 6ìœ¼ë¡œ ëŠ˜ë ¤ì¤Œ
                constraints["[PH1]"] = 6
                # ë‹¤ì‹œ ì„¤ì •
                sf.set_semantic_constraints(constraints)
            c. Validí•œ setìœ¼ë¡œë§Œ ì‚¬ìš©

        Error 2. kekulization failed
            ì˜ˆì‹œ. SMILES: NCCCCCOCC1c2n[c-]n[o+]c2CCN1c1ccccc1C(F)(F)F

            ìˆ˜ì • ë°©ë²• : í™”í•™ êµ¬ì¡° ì²˜ë¦¬ ì˜µì…˜ ë³€ê²½
            a. ê¸°ë³¸ ì„¤ì • ìœ ì§€
            b. íŠ¹ì • ì›ì†Œë§Œ ì œì•½ ìˆ˜ì •
                # íŠ¹ì • ì›ì†Œì˜ í™”í•™ êµ¬ì¡° ì²˜ë¦¬ ì˜µì…˜ ë³€ê²½
                Chem.Kekulize(mol, clearAromaticFlags=False)
                # ë‹¤ì‹œ ì„¤ì •
                Chem.Kekulize(mol, clearAromaticFlags=True)
            c. Validí•œ setìœ¼ë¡œë§Œ ì‚¬ìš©
        """
        for smi in self.df.smiles.tolist():
            # â€œ.â€ ë¡œ ì—°ê²°ëœ ë©€í‹°í”„ë˜ê·¸ë¨¼íŠ¸ SMILESì—ì„œ, ë¬¸ìì—´ ê¸¸ì´ê°€ ë” ê¸´ ì¡°ê°(fragment)ë§Œ ê³¨ë¼ë‚´ëŠ” ë¡œì§
            # => â€œmolA.molBâ€ ì‹ìœ¼ë¡œ ë‘ ë¶„ìê°€ ì—°ê²°ëœ SMILES ì¤‘ì—ì„œ, ê¸¸ì´ê°€ ë” ê¸´(=ì•„ë§ˆë„ ë¬´ê²Œê°€ í¬ê±°ë‚˜ ì£¼ëœ) ë¶„ìë§Œ ê³¨ë¼ë‚´ë„ë¡ í•˜ëŠ” ì „ì²˜ë¦¬ êµ¬ë¬¸
            # ì˜ˆì‹œ. â€œC1CCCCC1.C1CCCCC1â€ â†’ â€œC1CCCCC1â€ ë¡œ ë³€í™˜
            if "." in smi:
                smi = max(smi.split("."), key=len)
            

            # RDKitìœ¼ë¡œ ìœ íš¨ì„± ê²€ì‚¬ => ì¶”ê°€ by ì§„ì•„ ì„ ìƒë‹˜
            mol = Chem.MolFromSmiles(smi)
            if mol is None:
                self.invalid_smiles_rdkit.append(smi)
                # print(f"[INVALID] SMILES (RDKit ì‹¤íŒ¨): {com}")
                continue

            try:
                encoded_smiles = sf.encoder(smi)
                self.valid_smiles.append(smi)
            except sf.EncoderError as e:
                # encoder error ì²˜ë¦¬ - ì˜ˆì™¸ê°€ ë‚œ SMILESëŠ” error set ì €ì¥ ë° ë¡œê·¸ ìƒì„± í›„ ê³„ì† ì§„í–‰
                self.invalid_smiles_encoding.append(smi)
                # print(f"Error encoding {com}: {e}")
                continue

            # ì¸ì½”ë”©ëœ SMILESê°€ Noneì´ ì•„ë‹Œ ê²½ìš°, train_samples ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if encoded_smiles is not None:
                self.train_samples.append(encoded_smiles)
        
        print(f"Converted {len(self.train_samples)} SMILES entries to SELFIES")
        # print(len(self.train_samples))
        
        # ì•ŒíŒŒë²³ ìƒì„± : ëª¨ë“  í† í°ì„ ì¤‘ë³µ ì—†ì´ ëª¨ì•„ì„œ ì§‘í•©(set)ìœ¼ë¡œ ë§Œë“¦
        alphabet_set = sf.get_alphabet_from_selfies(self.train_samples)

        # íŠ¹ìˆ˜ í† í° ì¶”ê°€ (íŒ¨ë”©, ì‹œì‘ í† í°)
        """
        start_char("[^]")ì™€ pad_char("[nop]") 
            - SELFIES ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì—ì„œ íŠ¹ìˆ˜ í† í°(vocabulary special tokens) ìœ¼ë¡œ ì‚¬ìš©
            - ê¸°ë³¸ SELFIES ì•ŒíŒŒë²³ì—ëŠ” í¬í•¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ì ‘ ì¶”ê°€

        [nop] (Padding Token)
            - ìš©ë„: ëª¨ë“  SELFIES ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶° ì£¼ê¸° ìœ„í•´, ì§§ì€ ì‹œí€€ìŠ¤ì˜ ë’¤ì— ì±„ì›Œ ë„£ëŠ” í† í°
            - í•„ìš”ì„±
                - LSTMì´ë‚˜ QCBM ê°™ì€ ìˆœì°¨ ëª¨ë¸ì€ ì…ë ¥ ê¸¸ì´ê°€ ê³ ì •ë˜ì–´ ìˆê±°ë‚˜, ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì„ ë•Œ ê°™ì€ ê¸¸ì´ê°€ ë˜ì–´ì•¼ í•™ìŠµÂ·ì¶”ë¡ ì´ í¸ë¦¬
	            - "[nop]" í† í°ì„ ë§Œë‚˜ë©´ â€œì—¬ê¸°ëŠ” ì‹¤ì œ ë¶„ì ì •ë³´ê°€ ì•„ë‹˜â€ì„ ì•Œ ìˆ˜ ìˆìŒ

        [^] (Start-of-Sequence Token)
            - ìš©ë„: ì‹œí€€ìŠ¤ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ëª…ì‹œí•´ ì£¼ëŠ” í† í°
            - í•„ìš”ì„±
                - ëª¨ë¸ì´ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ì–´ë””ì„œë¶€í„° ì‹œì‘ë˜ëŠ”ì§€, í˜¹ì€ â€œì²« í† í°â€ì„ êµ¬ë¶„í•´ì•¼ í•  ë•Œ ì‚¬ìš©
	            - ë””ì½”ë”(ìƒ˜í”Œë§) ë‹¨ê³„ì—ì„œ â€œì²˜ìŒì— ë­ë¶€í„° ìƒì„±í• ì§€â€ë¥¼ ì§€ì‹œ
        """
        alphabet_set.add(pad_char)
        alphabet_set.add(start_char)
        self.alphabet = list(alphabet_set)


        # mapping char -> index and mapping index -> char
        # 2ê°œì˜ Dictionary ìƒì„±
        # enumerate() : ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ì™€ ê°’ì„ ë™ì‹œì— ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

        # char -> index : Model ì…ë ¥ìš© ìˆ«ì Seqë¡œ ë³€í™˜
        # ì˜ˆ: ("[C]", 0), ("[=O]", 1), â€¦
        self.char_to_index = dict(
            (c, i) for i, c in enumerate(self.alphabet) # (ì¸ë±ìŠ¤, í† í°) ìŒì„ (í† í°, ì¸ë±ìŠ¤) í˜•íƒœë¡œ ë’¤ì§‘ëŠ” ê³¼ì •
            )

        # index -> char : ë””ì½”ë”© ì‹œ ì‚¬ìš©
        # ì˜ˆ: (0, "[C]"), (1, "[=O]"), â€¦
        self.index_to_char = {v: k for k, v in self.char_to_index.items()}

        # ëª¨ë¸ì— ë„£ê¸° ì „ì— ì•Œì•„ì•¼ í•  ê¸°ë³¸ ì •ë³´ë“¤(ì–´íœ˜ í¬ê¸°, íŠ¹ìˆ˜ í† í°, ìµœëŒ€ ê¸¸ì´)â€ì„ í•œ ë²ˆì— ì´ˆê¸°í™”
        self.num_emd = len(self.char_to_index) # ë”•ì…”ë„ˆë¦¬ì— ë‹´ê¸´ í† í° ê°œìˆ˜(ì–´íœ˜ í¬ê¸°, vocabulary size)ë¥¼ ê°€ì ¸ì™€ num_emd(ì„ë² ë”© ì°¨ì› ìˆ˜)ë¡œ ì €ì¥
        self._pad_char = pad_char
        self._start_char = start_char
        self.data_length = max(map(len, self.train_samples)) # ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ì €ì¥

        # í›ˆë ¨ ìƒ˜í”Œ ì¤‘ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ 150%â€ë¥¼ ëŒ€ì²´ ê¸°ë³¸ê°’(fallback_max_length)ìœ¼ë¡œ ì„¤ì •
        fallback_max_length = int(len(max(self.train_samples, key=len)) * 1.5) 
        # max_lengthì˜ ì„¤ì •
        self._max_length = max_length if max_length is not None else fallback_max_length 

        # ë””ì½”ë”© ê³¼ì •ì—ì„œ ì¶”ì í•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        self.track_strings = []
        # self.encoded_samples_size = len(self.encoded_samples)

    # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì ë°˜í™˜
    def get_char_at(self, index: int) -> str:
        return self.index_to_char[index]

    # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    def get_chars_at(self, indices: t.List[int]) -> t.List[str]:
        return [self.get_char_at(index) for index in indices]

    # ë¬¸ìì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ë°˜í™˜
    def get_index_of(self, char: str) -> int:
        return self.char_to_index[char]

    # ë‹¨ì¼ SELFIESì— íŒ¨ë”© ì¶”ê°€
    def pad_selfie(self, selfie: str) -> str:
        """Add padding to a selfie such that the length of the padded selfie,
        matches that of the longest selfie in the dataset.
        """
        n_padding_tokens = self.max_length - sf.len_selfies(selfie)
        padding = self.pad_char * n_padding_tokens
        padded_selfie = selfie + padding

        return padded_selfie

    # train_samples ì „ì²´ì— íŒ¨ë”© ì ìš©
    @property
    def padded_selfies(self) -> t.List[str]:
        """Returns a list of selfies padded such that
        every string has the length of the longest string.
        """
        # faster appends and pops at edges
        padded_selfies = deque()
        for selfie in self.train_samples:
            padded_selfies.append(self.pad_selfie(selfie))

        return list(padded_selfies)

    # íŒ¨ë”© ë¬¸ì ë°˜í™˜
    @property
    def pad_char(self) -> str:
        """
        SELFIES íŒ¨ë”©ì— ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ ë°˜í™˜.
            - ì˜ˆ: '[nop]' ê°™ì€ í† í°ì´ íŒ¨ë”©ìš©ìœ¼ë¡œ ì‚¬ìš©ë¨
            - ë””ì½”ë”© ì‹œ ë¬´ì‹œë¨
        """
        return self._pad_char

    # ì‹œì‘ ë¬¸ì ë°˜í™˜
    @property
    def start_char(self) -> str:
        """
        SELFIES ì‹œí€€ìŠ¤ ì‹œì‘ ë¬¸ì ë°˜í™˜.
            - ì˜ˆ: '[^]' ê°™ì€ í† í°ì´ ì‹œì‘ í† í°ìœ¼ë¡œ ì‚¬ìš©ë¨
        """
        return self._start_char

    # íŒ¨ë”© ë¬¸ì ì¸ë±ìŠ¤ ë°˜í™˜
    @property
    def pad_char_index(self) -> int:
        """
        SELFIES íŒ¨ë”© ë¬¸ìì˜ ì¸ë±ìŠ¤ ë°˜í™˜.
            - ì˜ˆ: '[nop]' -> 57
            - ìˆ«ì encoding ëœ SELFIES ë²¡í„°ë¥¼ ë‹¤ë£° ë•Œ í•„ìš”
        """
        return self.get_index_of(self.pad_char)

    # ì‹œì‘ ë¬¸ì ì¸ë±ìŠ¤ ë°˜í™˜
    @property
    def start_char_index(self) -> int:
        """
        ì‹œì‘ ë¬¸ìì˜ ì¸ë±ìŠ¤ ë°˜í™˜.
            - ì˜ˆ: '[^]' -> 58
            - ì‹œí€€ìŠ¤ ìƒì„± ë˜ëŠ” ë””ì½”ë”© ì‹œ ì‚¬ìš©
        """
        return self.get_index_of(self.start_char)

    # ìµœëŒ€ ê¸¸ì´ ë°˜í™˜
    @property
    def max_length(self) -> int:
        """
        SELFIES ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ ë°˜í™˜.
            - í•™ìŠµ ì‹œ ëª¨ë¸ ì…ë ¥ ê¸¸ì´ë¥¼ ê³ ì •í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— í•„ìš”
            - ìë™ ê³„ì‚°ëœ fallback ê°’ ë˜ëŠ” ìˆ˜ë™ ì§€ì •ëœ max_length ì‚¬ìš©
        """
        return self._max_length

    # @property
    # def encoded_samples(self) -> np.ndarray:
    #     # Encode samples
    #     to_use = [
    #         sample
    #         for sample in self.train_samples
    #         if mm.verified_and_below(sample, self.max_length)
    #     ]
    #     encoded_samples = [
    #         mm.encode(sam, self.max_length, self.char_to_index) for sam in to_use
    #     ]
    #     return np.asarray(encoded_samples)

    # ì¸ì½”ë”©ëœ ìƒ˜í”Œ ë°˜í™˜ 
    @property
    def encoded_samples(self) -> np.ndarray:
        # Encode samples
        # to_use = [
        #     sample
        #     for sample in self.train_samples
        #     if mm.verified_and_below(sample, self.max_length)
        # ]
        """
        íŒ¨ë”©ëœ SELFIES ë¬¸ìì—´ë“¤ì„ ìˆ«ì ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ numpy ë°°ì—´ë¡œ ë°˜í™˜.
        - ì˜ˆ: '[C][=O][N]' â†’ [12, 45, 67]
        - ëª¨ë¸ ì…ë ¥ (X)ìœ¼ë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
        - enc_type='label': ìˆœì°¨ ì¸ë±ìŠ¤
            char_to_index = {'[C]': 0, '[=O]': 1, '[O]': 2, '[nop]': 3} ì¼ ë•Œ
            selfie = '[C][C][=O][O]' ğŸ‘‰ [0, 0, 1, 2]
        """
        encoded_samples = [
            sf.selfies_to_encoding(sel, self.char_to_index, enc_type="label")
            for sel in self.padded_selfies
        ]
        return np.asarray(encoded_samples)

    # @property
    # def one_hot_encoded_samples(self) -> np.ndarray:
    #     encoded_samples = self.encoded_samples
    #     n_samples = encoded_samples.shape[0]
    #     one_hot_encoding = np.zeros((n_samples, self.max_length, self.num_emd))
    #     for i_seq, seq in enumerate(encoded_samples):
    #         for i_element, element in enumerate(seq):
    #             one_hot_encoding[i_seq, i_element, element] = 1.0

    #     return one_hot_encoding

    # def decode_one_hot_smiles(self, smiles_one_hot: Tensor) -> t.List[str]:
    #     encoded_smiles_list = convert_to_numpy(smiles_one_hot).argmax(axis=2)
    #     return self.decode_smiles(encoded_smiles_list)

    # ìˆ«ì ì‹œí€€ìŠ¤ë¥¼ SELFIES ë¬¸ìì—´ë¡œ ë””ì½”ë”©
    def digit_to_selfies(self, encoded_selfies):
        """
        ìˆ«ì ì‹œí€€ìŠ¤ë¥¼ SELFIES ë¬¸ìì—´ë¡œ ë³µì› : ì¸ë±ìŠ¤ ë²¡í„° â†’ SELFIES ë¬¸ìì—´ 
            - ë””ì½”ë”©ì— ì‚¬ìš©ë¨
        """
        selfies = sf.encoding_to_selfies(
            encoded_selfies, self.index_to_char, enc_type="label"
        )
        return selfies

    # orquestra.qml.api 
    # Tensor -> torch.Tensor
    # convert_to_numpy -> numpy.ndarray

    # ìˆ«ì ì‹œí€€ìŠ¤ë¥¼ SMILES ë¬¸ìì—´ë¡œ ë””ì½”ë”©
    def decode_fn(self, encoded_selfies: t.Union[np.ndarray, torch.Tensor]) -> t.List[str]:
        """
        ìˆ«ì ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ (encoded_selfies) â†’ SELFIES ë¬¸ìì—´ â†’ SMILES ë¬¸ìì—´ë¡œ ë””ì½”ë”©.
        - SELFIES í† í°ì„ ì‹œì‘ ë¬¸ì ì œê±°í•˜ê³ 
        - sf.decoder()ë¥¼ í†µí•´ ìµœì¢… SMILES ë³µì›
        """

        # smiles are going to be one-hot encoded
        # encoded_sf_list = convert_to_numpy(encoded_selfies).tolist()

        """
        encoded_selfies
        : torch.Tensor of shape (batch_size, seq_len, vocab_size) or (batch_size, seq_len)
        """

        if isinstance(encoded_selfies, torch.Tensor):
            # 1) torch.Tensor â†’ NumPy array
            #    (detach, cpu ì´ë™ í›„ .numpy() ë¡œ ë³€í™˜) -> lists í˜•íƒœë¡œ ë³€í™˜
            numpy_array = encoded_selfies.detach().cpu().numpy()
            encoded_sf_list = numpy_array.tolist()
        elif isinstance(encoded_selfies, np.ndarray):
            encoded_sf_list = encoded_selfies.tolist()
        else:
            raise TypeError("Input must be a torch.Tensor or np.ndarray")

        # 3) ë””ì½”ë”© : ì •ìˆ˜ ì‹œí€€ìŠ¤ â†’ SELFIES ë¬¸ìì—´
        self.track_strings = []
        decoded_sf_list = list()

        for encoded_sf in encoded_sf_list:
            # ì •ìˆ˜ ì‹œí€€ìŠ¤ â†’ SELFIES ë¬¸ìì—´
            decoded_sf = self.digit_to_selfies(encoded_sf)
            # start token ì œê±°
            if self._start_char in decoded_sf:
                decoded_sf = decoded_sf.replace(self._start_char, "")
            # SELFIES â†’ SMILES
            decoded_smile = sf.decoder(decoded_sf)
            decoded_sf_list.append(decoded_smile)
        return decoded_sf_list

    # ìˆ«ì ì‹œí€€ìŠ¤ë¥¼ SELFIES ë¬¸ìì—´ë¡œ ë””ì½”ë”©
    def decode_char_selfies(self, encoded_selfies: t.Union[np.ndarray, torch.Tensor]) -> t.List[str]:
        """
        SELFIES ë¬¸ìì—´ í˜•ì‹ ìì²´ë¥¼ ë””ì½”ë”© -> SMILES ë¬¸ìì—´ë¡œ ë³€í™˜.
        â€» decode_fn ëŠ” ìˆ«ì ì¸ì½”ë”©ì„ SELFIES ë¡œ ë°”ê¾¸ê³  ì´ë¥¼ SMILES ë¡œ ë””ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
            - ìˆ«ì ì¸ì½”ë”©ì´ ì•„ë‹Œ ë¬¸ì ê¸°ë°˜ SELFIES ì…ë ¥ìš©
        """
        if isinstance(encoded_selfies, torch.Tensor):
            numpy_array = encoded_selfies.detach().cpu().numpy()
            encoded_sf_list = numpy_array.tolist()
        elif isinstance(encoded_selfies, np.ndarray):
            encoded_sf_list = encoded_selfies.tolist()
        else:
            raise TypeError("Input must be a torch.Tensor or np.ndarray")

        # ë””ì½”ë”© ê³¼ì •ì—ì„œ ì¶”ì í•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        decoded_sf_list = list()
        sf.decoder()
        for encoded_sf in encoded_sf_list:
            decoded_smile = sf.decoder(encoded_sf)
            decoded_sf_list.append(decoded_smile)
        return decoded_sf_list

    """
    # ì›ë³¸ ì½”ë“œ

    def decode_fn(self, encoded_selfies: Tensor) -> t.List[str]:
        # smiles are going to be one-hot encoded
        encoded_sf_list = convert_to_numpy(encoded_selfies).tolist()
        self.track_strings = []
        decoded_sf_list = list()
        for encoded_sf in encoded_sf_list:
            decoded_sf = self.digit_to_selfies(encoded_sf)
            if self._start_char in decoded_sf:
                decoded_sf = decoded_sf.replace(self._start_char, "")
            decoded_smile = sf.decoder(decoded_sf)
            decoded_sf_list.append(decoded_smile)
        return decoded_sf_list

    def decode_char_selfies(self, encoded_selfies: Tensor) -> t.List[str]:
        # smiles are going to be one-hot encoded
        encoded_sf_list = convert_to_numpy(encoded_selfies).tolist()
        decoded_sf_list = list()
        sf.decoder()
        for encoded_sf in encoded_sf_list:
            decoded_smile = sf.decoder(encoded_sf)
            decoded_sf_list.append(decoded_smile)
        return decoded_sf_list
    """
    
    
    # í™”í•™ êµ¬ì¡° ì‹œê°í™”
    def draw_smiles(self, smiles: str, molsPerRow: int = 0) -> Image:
        mols = [Chem.MolFromSmiles(s) for s in smiles]
        mols_filtered = [mol for mol in mols if mol is not None]
        if len(mols_filtered) == 0:
            raise RuntimeError("No Valid smiles were provided.")

        if molsPerRow <= 0:
            molsPerRow = len(mols)

        img = Draw.MolsToGridImage(mols, molsPerRow=molsPerRow, returnPNG=False)
        return img
    
#===============================================================================================================


def truncate_smiles(
    smiles: t.Iterable[str], padding_char: str = "_", min_length: int = 1
) -> t.List[str]:
    """
    SMILES ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒ¨ë”© ë¬¸ìê°€ ë“±ì¥í•˜ê¸° ì „ê¹Œì§€ì˜ ë¶€ë¶„ë§Œ ì˜ë¼ë‚¸ í›„,
    ìµœì†Œ ê¸¸ì´(min_length)ë³´ë‹¤ ì§§ì€ SMILESëŠ” ì œê±°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> smiles = ['cc2_N', 'cc2__', 'cc2']
        >>> truncate_smiles(smiles)
        ['cc2', 'cc2', 'cc2']

    ğŸ§  ì£¼ ìš©ë„:
        - LSTM ë“± ì‹œí€€ìŠ¤ ëª¨ë¸ì—ì„œ ì¶œë ¥ëœ SMILES ì¤‘ '_' ê°™ì€ íŒ¨ë”© ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš°
        - íŒ¨ë”© ì´í›„ ë‚´ìš©ì„ ì œê±°í•˜ì—¬ ìœ íš¨í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        - ë„ˆë¬´ ì§§ì€ ê²°ê³¼ëŠ” ì œê±°í•˜ì—¬ í’ˆì§ˆ í–¥ìƒ

    Args:
        smiles (Iterable[str]): ì²˜ë¦¬í•  SMILES ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (str iterable).
        padding_char (str, optional): íŒ¨ë”© ë¬¸ìë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì (ê¸°ë³¸ê°’: "_").
        min_length (int, optional): ìµœì†Œ SMILES ê¸¸ì´ (ê¸°ë³¸ê°’: 1). ì´ë³´ë‹¤ ì§§ìœ¼ë©´ ì œì™¸.

    Returns:
        List[str]: ì˜ë¦° í›„ ìœ íš¨í•œ SMILES ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """

    # ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ëˆ„ì í•˜ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    truncated_smiles = list()

    for smile in smiles:
        # padding_charê°€ ì¡´ì¬í•œë‹¤ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ê¹Œì§€ ìë¦„
        try:
            truncated_smile = smile[: smile.index(padding_char)]
        except ValueError:
            # íŒ¨ë”© ë¬¸ìê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì „ì²´ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            truncated_smile = smile

        # ìµœì†Œ ê¸¸ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if len(truncated_smile) >= min_length:
            truncated_smiles.append(truncated_smile)

    # ìµœì¢… ì˜ë¼ë‚¸ SMILES ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return list(truncated_smiles)